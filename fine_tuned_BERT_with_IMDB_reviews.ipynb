{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine tuned BERT with IMDB reviews.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMCrTC6K2kIAIimlbRllakZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akrawat912/Deep-Learning/blob/master/fine_tuned_BERT_with_IMDB_reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfrHu18vr1Ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for uploading kaggle.json file\n",
        "# it has kaggle user_name and unique key to all kaggle users for interacting with kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQnjlopdsNnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install the Kaggle API client.\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6UFSbLWT3Yq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "870f94d4-a2f2-4cd2-88d1-dfa627fabc74"
      },
      "source": [
        "!cp kaggle.json ~/.kaggle/\n",
        "# changing permission\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "#download dataset from kaggle \n",
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "imdb-dataset-of-50k-movie-reviews.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAYfDnoXsNp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create new dir to extract .zip\n",
        "!mkdir imdb_reviews\n",
        "# unzipping into fruits directory\n",
        "!unzip -uq imdb-dataset-of-50k-movie-reviews.zip -d imdb_reviews"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_ccSakusNtY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d27203e2-c93e-494f-8871-0c01211e240a"
      },
      "source": [
        "from glob import glob\n",
        "glob('imdb_reviews/*')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['imdb_reviews/IMDB Dataset.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHLLOGECsNwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-aF7Lc-uQcc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9ff6d07c-301e-4c73-a665-643b03b90660"
      },
      "source": [
        "data = pd.read_csv('imdb_reviews/IMDB Dataset.csv', nrows=10000)\n",
        "data.head(5)"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>clean_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>1</td>\n",
              "      <td>A wonderful little production. The filming tec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>0</td>\n",
              "      <td>Basically theres a family where a little boy (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>1</td>\n",
              "      <td>Petter Matteis \"Love in the Time of Money\" is ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  ...                                       clean_review\n",
              "0  One of the other reviewers has mentioned that ...  ...  One of the other reviewers has mentioned that ...\n",
              "1  A wonderful little production. <br /><br />The...  ...  A wonderful little production. The filming tec...\n",
              "2  I thought this was a wonderful way to spend ti...  ...  I thought this was a wonderful way to spend ti...\n",
              "3  Basically there's a family where a little boy ...  ...  Basically theres a family where a little boy (...\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  ...  Petter Matteis \"Love in the Time of Money\" is ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_XNXNQx28dZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "993d70a9-1dbd-4a4b-a4ff-4570d0ec4738"
      },
      "source": [
        "data.sentiment.unique()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['positive', 'negative'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAuSMc5u3BWZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8d36e6c-f418-4178-8bf4-7eec1c8f55cd"
      },
      "source": [
        "# target class positive as 1, negative as 0\n",
        "data.sentiment = data.sentiment.map({'positive':1, 'negative':0})\n",
        "data.sentiment.unique()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UhiMo1aubuq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "ee13ef28-4624-4779-f015-32d12c5f3ce9"
      },
      "source": [
        "data['review'][0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGjKBWWyyFrh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e5bfcac7-6c16-40ae-b565-4c3d33f32247"
      },
      "source": [
        "print('no. of samples: ',data.shape[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no. of samples:  50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_N8vY_Dy1Xj",
        "colab_type": "text"
      },
      "source": [
        "### PREPROCESSING reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS_EoCDezZba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    - Remove <br/>  tags\n",
        "    - Correct errors (eg. I\\'d, \\'s, \\'ve)\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    # Remove '<br/>'\n",
        "    text = re.sub(r'(<br />*)', ' ', text)\n",
        "\n",
        "    text = re.sub(r'(\\'+)', '', text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6fQYDjA0Hq9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "07fce0b7-17f8-4b32-efa3-b66a4cbcce39"
      },
      "source": [
        "data['review'][2]"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I\\'d laughed at one of Woody\\'s comedies in years (dare I say a decade?). While I\\'ve never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfO4pY1X0J1U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "f40c52a9-e07c-43b8-969f-dceb6f69879e"
      },
      "source": [
        "text_preprocessing(data['review'][2])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love. This was the most Id laughed at one of Woodys comedies in years (dare I say a decade?). While Ive never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman. This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUaTLzlb2jpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleaning reviews\n",
        "data['clean_review'] = data['review'].map(lambda a: text_preprocessing(a))"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ7y1f7s2nxN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ff3ac342-ea82-4928-a307-4d3ab719471b"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>clean_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>1</td>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>1</td>\n",
              "      <td>A wonderful little production. The filming tec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>1</td>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>0</td>\n",
              "      <td>Basically theres a family where a little boy (...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>1</td>\n",
              "      <td>Petter Matteis \"Love in the Time of Money\" is ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  ...                                       clean_review\n",
              "0  One of the other reviewers has mentioned that ...  ...  One of the other reviewers has mentioned that ...\n",
              "1  A wonderful little production. <br /><br />The...  ...  A wonderful little production. The filming tec...\n",
              "2  I thought this was a wonderful way to spend ti...  ...  I thought this was a wonderful way to spend ti...\n",
              "3  Basically there's a family where a little boy ...  ...  Basically theres a family where a little boy (...\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  ...  Petter Matteis \"Love in the Time of Money\" is ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOr05pfX2n1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = data.clean_review.values\n",
        "labels = data.sentiment.values"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeuiJR044qDA",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization & Input Formatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvkujfi72nv4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "f2188a81-e435-4949-f42a-12a29e5c7981"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.0rc4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-NB8o53nAKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "import torch"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPcxP2PY4tIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UOjJoRX4tKj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "dc1fecfe-664a-4786-faca-87d9bdc69ea0"
      },
      "source": [
        "# example tokenizer\n",
        "# Print the original sentence.\n",
        "print('Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  One of the other reviewers has mentioned that after watching just 1 Oz episode youll be hooked. They are right, as this is exactly what happened with me. The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word. It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away. I would say the main appeal of the show is due to the fact that it goes where other shows wouldnt dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesnt mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldnt say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards wholl be sold out for a nickel, inmates wholl kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n",
            "Tokenized:  ['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'oz', 'episode', 'you', '##ll', 'be', 'hooked', '.', 'they', 'are', 'right', ',', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me', '.', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'brutality', 'and', 'un', '##fl', '##in', '##ching', 'scenes', 'of', 'violence', ',', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', '.', 'trust', 'me', ',', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'tim', '##id', '.', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', ',', 'sex', 'or', 'violence', '.', 'its', 'is', 'hardcore', ',', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', '.', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'pen', '##ite', '##nta', '##ry', '.', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', ',', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inward', '##s', ',', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', '.', 'em', 'city', 'is', 'home', 'to', 'many', '.', '.', 'aryan', '##s', ',', 'muslims', ',', 'gangs', '##tas', ',', 'latino', '##s', ',', 'christians', ',', 'italians', ',', 'irish', 'and', 'more', '.', '.', '.', '.', 'so', 'sc', '##uf', '##fles', ',', 'death', 'stares', ',', 'dod', '##gy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away', '.', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'wouldn', '##t', 'dare', '.', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', ',', 'forget', 'charm', ',', 'forget', 'romance', '.', '.', '.', 'oz', 'doesn', '##t', 'mess', 'around', '.', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', ',', 'i', 'couldn', '##t', 'say', 'i', 'was', 'ready', 'for', 'it', ',', 'but', 'as', 'i', 'watched', 'more', ',', 'i', 'developed', 'a', 'taste', 'for', 'oz', ',', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', '.', 'not', 'just', 'violence', ',', 'but', 'injustice', '(', 'crooked', 'guards', 'who', '##ll', 'be', 'sold', 'out', 'for', 'a', 'nickel', ',', 'inmates', 'who', '##ll', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', ',', 'well', 'manner', '##ed', ',', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitch', '##es', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', ')', 'watching', 'oz', ',', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', '.', '.', '.', '.', 'that', '##s', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side', '.']\n",
            "Token IDs:  [2028, 1997, 1996, 2060, 15814, 2038, 3855, 2008, 2044, 3666, 2074, 1015, 11472, 2792, 2017, 3363, 2022, 13322, 1012, 2027, 2024, 2157, 1010, 2004, 2023, 2003, 3599, 2054, 3047, 2007, 2033, 1012, 1996, 2034, 2518, 2008, 4930, 2033, 2055, 11472, 2001, 2049, 24083, 1998, 4895, 10258, 2378, 8450, 5019, 1997, 4808, 1010, 2029, 2275, 1999, 2157, 2013, 1996, 2773, 2175, 1012, 3404, 2033, 1010, 2023, 2003, 2025, 1037, 2265, 2005, 1996, 8143, 18627, 2030, 5199, 3593, 1012, 2023, 2265, 8005, 2053, 17957, 2007, 12362, 2000, 5850, 1010, 3348, 2030, 4808, 1012, 2049, 2003, 13076, 1010, 1999, 1996, 4438, 2224, 1997, 1996, 2773, 1012, 2009, 2003, 2170, 11472, 2004, 2008, 2003, 1996, 8367, 2445, 2000, 1996, 17411, 4555, 3036, 2110, 7279, 4221, 12380, 2854, 1012, 2009, 7679, 3701, 2006, 14110, 2103, 1010, 2019, 6388, 2930, 1997, 1996, 3827, 2073, 2035, 1996, 4442, 2031, 3221, 21430, 1998, 2227, 20546, 2015, 1010, 2061, 9394, 2003, 2025, 2152, 2006, 1996, 11376, 1012, 7861, 2103, 2003, 2188, 2000, 2116, 1012, 1012, 26030, 2015, 1010, 7486, 1010, 18542, 10230, 1010, 7402, 2015, 1010, 8135, 1010, 16773, 1010, 3493, 1998, 2062, 1012, 1012, 1012, 1012, 2061, 8040, 16093, 28331, 1010, 2331, 14020, 1010, 26489, 6292, 24069, 1998, 22824, 10540, 2024, 2196, 2521, 2185, 1012, 1045, 2052, 2360, 1996, 2364, 5574, 1997, 1996, 2265, 2003, 2349, 2000, 1996, 2755, 2008, 2009, 3632, 2073, 2060, 3065, 2876, 2102, 8108, 1012, 5293, 3492, 4620, 4993, 2005, 7731, 9501, 1010, 5293, 11084, 1010, 5293, 7472, 1012, 1012, 1012, 11472, 2987, 2102, 6752, 2105, 1012, 1996, 2034, 2792, 1045, 2412, 2387, 4930, 2033, 2004, 2061, 11808, 2009, 2001, 16524, 1010, 1045, 2481, 2102, 2360, 1045, 2001, 3201, 2005, 2009, 1010, 2021, 2004, 1045, 3427, 2062, 1010, 1045, 2764, 1037, 5510, 2005, 11472, 1010, 1998, 2288, 17730, 2000, 1996, 2152, 3798, 1997, 8425, 4808, 1012, 2025, 2074, 4808, 1010, 2021, 21321, 1006, 15274, 4932, 2040, 3363, 2022, 2853, 2041, 2005, 1037, 15519, 1010, 13187, 2040, 3363, 3102, 2006, 2344, 1998, 2131, 2185, 2007, 2009, 1010, 2092, 5450, 2098, 1010, 2690, 2465, 13187, 2108, 2357, 2046, 3827, 7743, 2229, 2349, 2000, 2037, 3768, 1997, 2395, 4813, 2030, 3827, 3325, 1007, 3666, 11472, 1010, 2017, 2089, 2468, 6625, 2007, 2054, 2003, 8796, 10523, 1012, 1012, 1012, 1012, 2008, 2015, 2065, 2017, 2064, 2131, 1999, 3543, 2007, 2115, 9904, 2217, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy1quAaLkjF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing_for_bert(data):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts(sentences) to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=sent,  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=128,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True,      # Return attention mask\n",
        "            truncation=True\n",
        "            )\n",
        "        \n",
        "        # Add the encoded sentence to the list.\n",
        "        input_ids.append(encoded_sent['input_ids'])\n",
        "        # And its attention mask (simply differentiates padding from non-padding).\n",
        "        attention_masks.append(encoded_sent['attention_mask'])\n",
        "      # Convert the lists into tensors.\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GcNEbn8kjJz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "e312ba99-71e5-46cd-e345-41b04eef1db3"
      },
      "source": [
        "# Print sentence 0 and its encoded token ids\n",
        "token_ids = preprocessing_for_bert([sentences[0]])[0]\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs: ', token_ids)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  One of the other reviewers has mentioned that after watching just 1 Oz episode youll be hooked. They are right, as this is exactly what happened with me. The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word. It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away. I would say the main appeal of the show is due to the fact that it goes where other shows wouldnt dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesnt mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldnt say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards wholl be sold out for a nickel, inmates wholl kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n",
            "Token IDs:  tensor([[  101,  2028,  1997,  1996,  2060, 15814,  2038,  3855,  2008,  2044,\n",
            "          3666,  2074,  1015, 11472,  2792,  2017,  3363,  2022, 13322,  1012,\n",
            "          2027,  2024,  2157,  1010,  2004,  2023,  2003,  3599,  2054,  3047,\n",
            "          2007,  2033,  1012,  1996,  2034,  2518,  2008,  4930,  2033,  2055,\n",
            "         11472,  2001,  2049, 24083,  1998,  4895, 10258,  2378,  8450,  5019,\n",
            "          1997,  4808,  1010,  2029,  2275,  1999,  2157,  2013,  1996,  2773,\n",
            "          2175,  1012,  3404,  2033,  1010,  2023,  2003,  2025,  1037,  2265,\n",
            "          2005,  1996,  8143, 18627,  2030,  5199,  3593,  1012,  2023,  2265,\n",
            "          8005,  2053, 17957,  2007, 12362,  2000,  5850,  1010,  3348,  2030,\n",
            "          4808,  1012,  2049,  2003, 13076,  1010,  1999,  1996,  4438,  2224,\n",
            "          1997,  1996,  2773,  1012,  2009,  2003,  2170, 11472,  2004,  2008,\n",
            "          2003,  1996,  8367,  2445,  2000,  1996, 17411,  4555,  3036,  2110,\n",
            "          7279,  4221, 12380,  2854,  1012,  2009,  7679,   102]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh1XFA_hpK0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 0's are padded to max_len, This mask tells the “Self-Attention” mechanism in BERT not to\n",
        "# incorporate these PAD tokens into its interpretation of the sentence."
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECkbromjt4aF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6f5632bd-06bd-43dd-c2ad-c2c79481e5fa"
      },
      "source": [
        "input_ids, attention_masks = preprocessing_for_bert(sentences)\n",
        "labels = torch.tensor(labels)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYS4-XFwvWI1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "143bedc6-17d2-49ff-eb26-66cc3849a401"
      },
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWXTUKJ4s4Vs",
        "colab_type": "text"
      },
      "source": [
        "### Training & Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vzeMDv-qjr6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "eeeedff4-9faf-493b-b7a4-44ff75712ad9"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "# Create a 90-10 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9,000 training samples\n",
            "1,000 validation samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SdGv9MP38C-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "batch_size = 32\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtMy-RpX7sJc",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6YPRYxiXlim",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T77Nhhtxun0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "def initialize_model(epochs=4):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve3btQ9Dun3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 100 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJHxd13y2RIl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "f7cccddd-fdeb-4781-bcb9-940f970159da"
      },
      "source": [
        "set_seed()    # Set seed for reproducibility, default 42\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
        "train(bert_classifier, train_dataloader, validation_dataloader, epochs=2, evaluation=True)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.603416   |     -      |     -     |   27.44  \n",
            "   1    |   40    |   0.392103   |     -      |     -     |   26.13  \n",
            "   1    |   60    |   0.368296   |     -      |     -     |   26.07  \n",
            "   1    |   80    |   0.352462   |     -      |     -     |   26.04  \n",
            "   1    |   100   |   0.361095   |     -      |     -     |   26.03  \n",
            "   1    |   120   |   0.398871   |     -      |     -     |   26.05  \n",
            "   1    |   140   |   0.313055   |     -      |     -     |   25.99  \n",
            "   1    |   160   |   0.379125   |     -      |     -     |   26.04  \n",
            "   1    |   180   |   0.342183   |     -      |     -     |   26.00  \n",
            "   1    |   200   |   0.360162   |     -      |     -     |   26.01  \n",
            "   1    |   220   |   0.284981   |     -      |     -     |   26.00  \n",
            "   1    |   240   |   0.319527   |     -      |     -     |   26.02  \n",
            "   1    |   260   |   0.289080   |     -      |     -     |   26.02  \n",
            "   1    |   280   |   0.271569   |     -      |     -     |   26.03  \n",
            "   1    |   281   |   0.592701   |     -      |     -     |   1.10   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.361399   |  0.283337  |   87.79   |  381.00  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.178346   |     -      |     -     |   27.28  \n",
            "   2    |   40    |   0.180881   |     -      |     -     |   25.99  \n",
            "   2    |   60    |   0.174620   |     -      |     -     |   26.03  \n",
            "   2    |   80    |   0.167896   |     -      |     -     |   26.01  \n",
            "   2    |   100   |   0.220834   |     -      |     -     |   26.02  \n",
            "   2    |   120   |   0.151375   |     -      |     -     |   25.99  \n",
            "   2    |   140   |   0.181747   |     -      |     -     |   26.02  \n",
            "   2    |   160   |   0.128679   |     -      |     -     |   26.02  \n",
            "   2    |   180   |   0.147600   |     -      |     -     |   26.02  \n",
            "   2    |   200   |   0.200494   |     -      |     -     |   26.04  \n",
            "   2    |   220   |   0.122964   |     -      |     -     |   26.04  \n",
            "   2    |   240   |   0.152899   |     -      |     -     |   25.99  \n",
            "   2    |   260   |   0.156269   |     -      |     -     |   26.02  \n",
            "   2    |   280   |   0.118280   |     -      |     -     |   26.00  \n",
            "   2    |   281   |   0.037415   |     -      |     -     |   0.41   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.162672   |  0.333879  |   87.40   |  379.93  \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZx6osHW83Xq",
        "colab_type": "text"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gji2mJPS819D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYZAIDfJIhzD",
        "colab_type": "text"
      },
      "source": [
        "### Prediction on unseen data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElmK0-eEJCvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we get first 10k samples for train and validation, we getting last 5k sample which is unseen\n",
        "test_df = pd.read_csv('imdb_reviews/IMDB Dataset.csv', ).tail(5000)"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGRpPYv_Ie6I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ee425f20-7089-479f-86a7-711bc314186c"
      },
      "source": [
        "test_df = test_df.reset_index()\n",
        "del(test_df['index'])\n",
        "test_df.head()"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What I enjoyed most in this film was the scene...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MacArthur is a great movie with a great story ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What can I say? I ignored the reviews and went...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A pretty transparent attempt to wring cash out...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Even though the book wasn't strictly accurate ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  What I enjoyed most in this film was the scene...  positive\n",
              "1  MacArthur is a great movie with a great story ...  positive\n",
              "2  What can I say? I ignored the reviews and went...  negative\n",
              "3  A pretty transparent attempt to wring cash out...  negative\n",
              "4  Even though the book wasn't strictly accurate ...  negative"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMGwCHsUI-mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleaning reviews\n",
        "test_df['clean_review'] = test_df['review'].map(lambda a: text_preprocessing(a))"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqoUNsxWN9EX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df.sentiment = test_df.sentiment.map({'positive':1, 'negative':0})"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pLQItjxI-qR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_to_predict = test_df.clean_review.values"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT8Hg-7jLzFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ip_ids, att_masks = preprocessing_for_bert(sent_to_predict)"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkpzstcKLzIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = TensorDataset(ip_ids, att_masks)\n",
        "test__dataloader = DataLoader(\n",
        "            test_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMnMDcQ6LzDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predicting\n",
        "probs = bert_predict(bert_classifier, test__dataloader)"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDazCAHCHDm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy_score(y_true, y_pred):\n",
        "  \"\"\"custom function to calculate accuracy between binary classes \"\"\"\n",
        "  return (y_true == y_pred).sum() / float(len(y_true))"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1kcl_ZJIRIf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7e95ce7-8afc-4331-94cf-436e33bb72ce"
      },
      "source": [
        "y_pred = [np.argmax(i) for i in probs] # predicted class \n",
        "y_true = test_df.iloc[:,1].values #actual class, 1 index is of 'sentiment' column.\n",
        "\n",
        "accuracy_score(y_true, y_pred)"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8786"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcnlkTEENeRn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "2810b51e-80fa-43b5-836e-d2f2035170e0"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "data = confusion_matrix(y_true, y_pred) \n",
        "df_cm = pd.DataFrame(data, columns=np.unique(y_true), index = np.unique(y_true))\n",
        "df_cm.index.name = 'Actual'\n",
        "df_cm.columns.name = 'Predicted'\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.set(font_scale=1.4)#for label size\n",
        "sns.heatmap(df_cm, cmap=\"Blues\", annot=True, annot_kws={\"size\": 20})\n",
        "plt.show()"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAloAAAG5CAYAAABWY5pbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd5xU1dnA8d8uKKKUBRUBpSjoETQ2ir1XREGNHQt2Yve1xYoaS1QSu7HEGrsxKiaIBRWMgmJXhKMgHVtwKSpK2/ePmcUtA3svzLCU3zef+Yxz77lnzmxg5+F5zjm3qKysDEmSJOVfcW0PQJIkaUVloCVJklQgBlqSJEkFYqAlSZJUIAZakiRJBWKgJUmSVCB1a3sAhVJ/y9Pdt0Jayr4fdlttD0FaaTWoV1S0NN8vn9+zsz68famOfWkyoyVJklQgK2xGS5IkFVCRuZokDLQkSVJ6S7dSudwyHJUkSSoQM1qSJCk9S4eJGGhJkqT0LB0mYjgqSZJUIGa0JElSepYOEzHQkiRJ6Vk6TMRwVJIkqUDMaEmSpPQsHSZioCVJktKzdJiI4agkSVKBmNGSJEnpWTpMxEBLkiSlZ+kwEcNRSZKkAjGjJUmS0rN0mIiBliRJSs/SYSKGo5IkSQViRkuSJKVn6TARAy1JkpSegVYi/pQkSZIKxIyWJElKr9jJ8EkYaEmSpPQsHSbiT0mSJKlAzGhJkqT03EcrEQMtSZKUnqXDRPwpSZIkFYgZLUmSlJ6lw0QMtCRJUnqWDhMx0JIkSemZ0UrEcFSSJKlAzGhJkqT0LB0mYqAlSZLSs3SYiIGWJElaboQQDgF6AZ2ApsAY4G/A3THG+RXadQOuAToCk4GbY4y35ejvPOA0oDkwArgwxjioSpuGwI3AwcBqwOvAGTHGcTWN17yfJElKr6g4f490zgV+Bc4H9gOeA24Fri9vEELYFugPfAh0Ax4Abg4h9KnYUTbIuha4A+gOfAn8J4SweZX3fBzoAZwBHAa0BAaFEFavabBmtCRJUnq1VzrcP8b4fYXXr4cQGgCnhxAujTH+ClwOfBBjPKFCm9ZA3xDCPTHG+SGEesClZDJd/QBCCIOBT4FLgEOzx7YmE4R1jzEOyB77lEwmrTdw56IGa0ZLkiQtN6oEWeU+JFPSa5oNoHYDnqzS5jEy5cGtsq+3AxoDT1Toex7wFNAthFAeSe4LTAcGVmg3AXgre26RzGhJkqT08rjqMIRQApTkODUtxjgtQRc7Aj8A3wEBWBX4vEqbEdnnjYH3gA7Z1yNztGsArAtMyrYbVXH+V4V2e9c0MDNakiQpvfzO0TobGJvjcXZNwwghdAaOA27KZqSaZE9VDdBKs89Ns89NgF9jjLMStMsV7JVWaLNQZrQkSVJtuxl4MMfxRWazQgjNgWeAd6kwGX5ZYqAlSZLSy+Nk+Gx5MEmJcIEQQmPgReBnoEeMcU72VHlGqmopsjzT9UOFdvVCCKvFGH+poV3rHENoUqHNQlk6lCRJ6dXe9g6EEFYjs31DM2CfGOPUCqfHALP5bQ5WuY7Z51HZ5/K5WbnazSSz91Z5u1BhcnzFdqOogYGWJElaboQQ6pJZGbgZ0C3GOL7i+ez2Dq+R3Z6hgiOAb4APsq/fJrOa8LAKfdfJXjcwxliWPTyATHZs7wrtWgE7ZM8tkqVDSZKUXu3to3UHsD9wAbB6CGGbCuc+jzHOAK4ChoQQ7gUeBbYHTgJOK189GGP8NYRwNXBtCOF7MgHYiUA74MjyDmOM74QQ/gPcF0I4FyjvfwK555VVYqAlSZLSq72bSpdnlm7IcW5X4I0Y49AQQk8yu74fA0wBzokx3lWxcYyxXwgB4ExgHTJbNnSPMX5cpd8jgH5kNietR+YWPIfEGH+uabBFZWVlNbVZLtXf8vQV84NJy7Dvh1W7jZikpaRBvaWbYqp/4N/z9j0769kTV9g7VJvRkiRJ6dVe6XC5YqAlSZJSKzLQSsRVh5IkSQViRkuSJKVmRisZAy1JkpSecVYilg4lSZIKxIyWJElKzdJhMgZakiQpNQOtZCwdSpIkFYgZLUmSlJoZrWQMtCRJUmoGWslYOpQkSSoQM1qSJCk9E1qJGGhJkqTULB0mY+lQkiSpQMxoSZKk1MxoJWOgJUmSUjPQSsbSoSRJUoGY0ZIkSamZ0UrGQEuSJKVnnJWIpUNJkqQCMaMlSZJSs3SYjIGWJElKzUArGUuHkiRJBWJGS5IkpWZGKxkDLUmSlJ5xViKWDiVJkgrEjJYkSUrN0mEyBlqSJCk1A61kLB1KkiQViBktSZKUmhmtZAy0JElSagZayVg6lCRJKhAzWpIkKT0TWokYaEmSpNQsHSZj6VCSJKlAzGhJkqTUaiujFUJoD5wHbANsCoyKMW5apU3ZIrrYNsY4LNvuDWDnHG26xBjfq9DfKsBVwLFACTAcOCvG+FFN4zXQkiRJqdVi6XAToDvwDpnKXK7q3LY5jt0EbAC8V+X4W2QCt4pG5rj2GOBcYBxwATAohPC7GOOURQ3WQEuSJKVXe1O0XogxPg8QQngQ6Fy1QXnGqlwIoQTYErgnxji3SvNpVdtXuXZdoA9wZozx3uyxYcBY4GwyQddCGWitZJo2XoMeu21Gtx02ZZP2LWnZrDGz58xjxOgpPNx/GA8/P4yyskVlXPPbz9Ky8QbNufSUfdmx84Y0WmM1Jnz9A0+/9D79HniFX36dU6nteuuUcN7xe7FVh9a0atGUJo3q88P0n/lq4vc89PwwHh/wLnPnzq+lT6Ll2a039ePzEZ8xYfw4pk0rpV691WjRsiW77Lo7hx7Ri5KSJku1n6XhqzGjuftvt/P+8Hf56acfadGiJXvtsy+9TziZ1VZbrVLbCePH8dqgVxj61n+ZOGE8U6dOpVGjRvxus8054qhj6NJ1m1r6FFqWxBgX5xfwIUA94JHFuHYvoA7wZIUxzAwh/BvYlxoCraJl6cswn+pvefqK+cGW0IkH78BtlxzO199PZ/DwL5j4TSnNmjak5+6bU9JwdZ599UOOPP++pdbP0tBl0za8eM+ZrFK3Ds+++hGTvilll64b0WmTNrz94Ri6nXIbs+f89g+cHTttyNM3nczwz8YxdvJUSqf/RNPGa7D39h1p1aIpb7wb2e/UO5g3z2Crqu+H3VbbQ1imbb3V79i4Q0c2aNeOJk3X5JdZP/PpJx/z+YjPWLtZMx585EmaN2+x1PoptE8/+Zg+J/Zm7ty57L7nXjRv3oLh7w7j8xGfsfmWW3HXvQ+y6qqrLmh/0QX/x8sDB7BBu/ZsseVWNGrcmPHjxjHkjdeYN28e5114MUf0OqYWP9GyrUG9pVvLa31G/7x9z064rcdijb08o1V1jlaOdoOB5jHGUOX4G0AnMvm5umTKin1jjIMqtLkBOCbG2LzKtecD1wCrLSr4M6O1kvly/Hf8/qy7ePHNEZUyTn1v78+b/zifA/fYkgN234LnBi16fl+++lkcO3bakJf/fhYnXf4PHnnhnUW2LS4u4u4rj2KN+vU4+Oy7+c/gT4HM3IJHbzieA/fYkjOP2pV+D7yy4JphH39Fi50vqJaRq1u3mH/feTq7dA0csNvmPPPKh3n/bFqxDRn6HvXq1at2/I5bb+L+v9/NA3+/h4su7bvU+knrveHvcMoJx9L3T9fSo+dBi2w7b948rrz8Yn75ZRZ/veVOdt51NwDmz5/PH887m0Gvvsyj/3iQ4044ecE1226/A8cedyIbd+hYqa/333uXU08+gVv+eiN77LUPa6/dLO+fTenlc45WtrRXkuPUtBjjtCXsuzWwI3BFjtODgX8AXwLrkCkFvhxC2DPG+Fq2TRMg1xhKgVWABsCMhb2/2zusZAYP/4IBQz6rFkR8O3Um9/7zvwDs1HnDgvZz6D6dGHjPmXw95AZKh93Eh89cyoUn7s2qq+Q/7t+x04Z02KAFb77/5YIgC6CsrIyLb34OyGTnKpozd17OsufcufN54fVPAGjX2l/0Si9XcASw597dAJg4YXxB+xk44N+cfMIx7Lx9F7btvBm/77kvf7/nb8yePTvR+6bx/nvDGfvVGLbq1HlBkAVQXFzMmf93PgDPPP1Epb9rPXoeVC3IAujUuSudunRhzpw5fPKR/8BZQZ1NZs5T1cfZeej7SDIZq2plwxhj3xjjfTHGITHGp4HdyARdV+ThfQEzWqpg7tx5lZ4L0c9dfXtx7AHbMumbUp4b9BHTZ86i6+/acsVp+7Nr10D3P9ye15LcLl02AuDlt6suIIFxk6fyxbhv2ajtOqy/3lqMnfS/RfZVXFzE3jtsAsBnX07O2xilIYNfB6D9RhsVrJ8rL7+Y/s/9i3XWac7ue+xFg4YN+eyTj/nb7bfw7jtDufPu+6lbN39fCe+9m5lbvO32O1Y7t956rWjTpi3jx49j0qSJtGrVusb+6tZdBYA6eRyjlkyeVx3eDDyY4/gSZbOyegFDY4xf1dQwxvhrCOF54PQKh0vJnW1rAswBflxUn7X6JzaE0AHoBmwMNM0e/gEYBQyIMY6qrbGtbOrUKebI/bYGcgcl+ejnqP235tgDtuX5QR/R+5KHKk1Cv+SUfbm0z770OXQn7nj8jcV+/6o2apvJPI0e/13O86MnfM9GbddhwzbNqgVaa5asQZ/DdqaoCNZq0oDdt9mY9q2b8cSA4QwY8lnexqiVz8MP3sesn3/mxx9/5PMRn/HRh++z4UaB444/ueaLF6Of/s//i/7P/Ytdd9+Tq6+7sdIk9LvvvI177rqDp554jCOPyt/8p3HjxgLQpk3bnOdbtWnD+PHjmDBuXI2B1tdTJjP8naGstlp9tupUbYGZakk+A61seTAfQVUlIYQtyOy1ddoSdDMSaBZCaBpj/KHC8Y7AFzVNzq+VQCuEUB+4DzgMmA2MIRMxQiboOhq4MYTwBHBCjPGX2hjnyuTqM3uy6YYtefHNz3h16OIHWovq57Qjd2HOnHmccsWj1Vb6XXfvi/Q5bCcO37dzXgOtRg3qAzD9x1k5z8/IHi9pWL/auTVLGnBpn30XvJ4/fz43PfQql9/eP2/j08rpkYceYOrU3wL77bbfkSuuvo4mTZsu4qrF7+eJR/9Bnbp16XvlNdVW+p14yqk8+cSjDBzwQl4DrR9/nAlAg4YNc55v0CBzfObMhU5tAWD27NlcctH5zJ49m7POOZ9GjRrnbYxaKfQik3V6sqaGACGEesABZDYkLfcyMB84FLgr264BsD9wf0191lZG63pgT+Ao4JkYY6UJAiGEVYGDgFuzbc9a6iNciZx6xM6cfczujPrqG0649OGC9FN/tVXYbKN1+d+0nzi91645r589Zy5h/UqLOnjp3rMWOtfr3quO5t6rjq50bMh7X7L3Sbcs9meo6Itx31J/y9MpLi5i3WYl9Nh1cy77Q3e227IdB57xN0pn/JyX99HK5+XXM/MYp079Hx9/9CG33/wXjjz0QG6+7S46dNwkr/3MmjWLL+IoSkqa8NgjD+XsZ9VVV2XsV2MqHTv5+KN5/73hOdtfednFXHnZxZWOderchXvu/0fisScxb948Lrv4Aj7+8AP22mdfju59fF771xKqpX20Qgirk9lWAaAN0CiEcHD29fAY4/hsu2LgCGBgjHFqjn52BM4HniWzCWlzMvHGBsAp5e1ijJNDCHcB14cQ5gLjyWxwWkSm5LlItRVoHQ6cE2N8PNfJbOD1RHbL+79goFUwfQ7bib9ccAifj/mafU+5dbGDh5r6adJwdYqLi2nWtGGlLFFN/tF/GEPe+7LSsTYtm3J0j2144fWP+ThWnis1fkrlv0vlGavGDapnrOC3jNe0mbkzXgDz55cx8ZtS7nj8Db77YSYP//k4Lv9Dd865/unEn0PKZc0112K33fekQ4eOHLj/PvS95I889ewLee1n5owZlJWVUVr6A/fcdUfiPvfreSCdOnetdGzKlMn8u/9z7Lzr7oSwcaVzLdZdt9Lr8ozVjzNn5uy/POPVsGGjnOfnzZvHZRedz6svD2TPvbvxp2tv8CbGy5ha/P+jGVD1F3D56+P4ba7XLsC6wP8tpJ+vgVWBa4E1gZ+BYcAuMca3qrQ9h8xcrKuBxmQyXnvUtCs81F6gVR/4NkG7b7NtVQCnH7kLN55/MJ99OYV9T7mV70sXOZ9vifopL919OHIi2x15feK+c23fsGOnDTm6xzb0f/2TGrd3+GJcZm5W+za5Vwm2b702kNmuIomX3hqRGUOClZlSUi1arssG7doTR42ktLSUJk0Wb8PRXP00aNgAgLBxRx576l+J+8q1fcN7w9/h3/2fY5fddq9xe4e2bdcHYPz4cTnPTxyfWRnZum3baufmzJnDpdkga5999+Oqa66nTp06iceuFVuMcRwJ8mnZ7RkW2i7GOBrYJ+F7zgH+mH2kUlvbO7wFXB5CWOhvk+y5y4A3l9qoViLn9t6DG88/mI9GTWSfk29Z7CAraT8/zZrNiNFT6NiuOU0arb4kQ0/ljeFfALDXdh2qnWu77pps1HYdxk+ZWuOKw3Itm2UWnsx1s1Ll2fffZYL9OnWW7Ndy1X5WX30N2rXbkK/GfMn06Xmfa7xQnbO7uA99q/qv8EmTJjJ+/DhatGzJeuu1qnRuzpzZXHje2bz68kC679+TP117g0HWMqqoqChvjxVZbQVapwPrAxNCCM+FEK4NIVyQfVwbQniWTA20DXBmLY1xhfXHk/bh6rMO4P3PJ7DvKbcxddpPC21bt27xgu0PlqQfgFsfeY16q67C3Vf0ylnKK2lYny02Xi/9B1qEN9//kpFffc2OnTak+86/W3C8qKiIa87qCcDfs/t+ldti4/UoLq7+F3+N+qvS7/zMNICBb47I6zi14hs/biwzc5TR5s+fzx233sQPP0xl8y22XDDZe86cOYwd+xUTJ05Yon4Aeh3Tmzlz5nDl5Zcwc0b1yeczZkxn5Of5/TPdqXMX1t+gHR+8/x6DX39twfH58+dz6039APj9IYdX+pKdPXs25559BoNfH0TPAw/mij9dR3Gx2z0uq4qK8vdYkdVK6TDGODqEsAmZmzR2A04gsx8FZFYfjiRTB707xji9Nsa4ouq1/9b0PXU/5s6dx9sfjObUI3ap1mb8lKkLSnIt1y7h42cvY/yUqWzcve9i9wPw8PPD2LJDa/octhMjXmjPq0NHMvGbH2jSaA3arrsmO2zVjof7v8OZ1zyRt887f34Zp/R9hBfvOZPHbjyBZ1/9iInf/MCuXcOCW/Dc+sjrla65+ORubLPFBgz7eCyTvinl51mzWa95E/baviNNGq3O0I/GcOP9L+VtjFo5vPXmEG6/9a9ssWUnWq67Lo1LSvhh6lTef284kydNZM211ubSvn9a0P77777l4J770qJlS/498LXF7geg54G/Z+TnI3j6ycfo2X1PttluB5q3aMmM6dOYMnkyH7w/nB4HHESHjlfm7fPWqVOHvlddS58Te3PBuWdlbsHToiXD3xm64BY8vY7uXemaa//Ul7feHExJkyY0a9aMe3PMKevUpSudu2ydt3FKhVZr+2hlA6jrsw8tJW1brglA3bp1OOOo3XK2GfLelzXOfVrcfs7581O8/NbnnHjwDuy6daCkYeaGzZO++YGbHhrE4wNyr3JaEsM/G88OR93IZX32ZfdtNqbhGvWY8HUp19w9gH4PvFLpPocA9//rbX78+Vc6b9qWnTptyOqrrUrpzJ/5cOQEnnn5Qx56fqj3OVRqXbfZlp4Tf89HH37AqFGf8+PMmaxWvz5t2rSl+349OLzX0TRunGtPxPz088dLLme7HXbkmaef4N1hQ5k5cyaNGzemeYsWHNP7BLrt1yPvn/l3m23OPx5/mrvuvI1hQ9/i559+okXLlpx0yqn0PuHkSvc5BJgyeRIA00pLuffuO3P2eTIYaC0jVvSSX754U2lJeeNNpaXas7RvKr3RBQPz9j37xQ37rLBRm8VvSZKkAvGmUZIkKTVLh8kYaEmSpNSMs5KxdChJklQgZrQkSVJqufYbVHUGWpIkKTVLh8lYOpQkSSoQM1qSJCk1Vx0mY6AlSZJSM85KxtKhJElSgZjRkiRJqVk6TMZAS5IkpWaglYylQ0mSpAIxoyVJklIzoZWMgZYkSUrN0mEylg4lSZIKxIyWJElKzYRWMgZakiQpNUuHyVg6lCRJKhAzWpIkKTUTWskYaEmSpNQsHSZj6VCSJKlAzGhJkqTUTGglY6AlSZJSs3SYjKVDSZKkAjGjJUmSUjOhlYyBliRJSs3SYTKWDiVJkgrEjJYkSUrNhFYyBlqSJCk1S4fJWDqUJEkqEDNakiQpNRNayRhoSZKk1GqrdBhCaA+cB2wDbAqMijFuWqXNg8CxOS4/JMb4zyptzwNOA5oDI4ALY4yDqrRpCNwIHAysBrwOnBFjHFfTeC0dSpKk5ckmQHdgNPD5Itp9BWxb5fFaxQbZIOta4I5sn18C/wkhbF6lr8eBHsAZwGFAS2BQCGH1mgZrRkuSJKVWi5PhX4gxPg8LMledF9JuVoxx2MI6CSHUAy4Fbo4x9sseGwx8ClwCHJo9tjWZIKx7jHFA9tinwBigN3DnogZrRkuSJKVWVJS/Rxoxxvl5+gjbAY2BJyr0PQ94CugWQigf2b7AdGBghXYTgLey5xbJjJYkSapVIYQSoCTHqWkxxmmL2W27EMI0YA3gM+DPMcYnK5zvkH0eWeW6EUADYF1gUrbdqBwB3ghg75oGYUZLkiSlVlRUlLcHcDYwNsfj7MUc3odkJswfQGYC+yTgiRBC7wptmgC/xhhnVbm2NPvctEK7XMFeaYU2C2VGS5IkpZbnKVo3Aw/mOL5Y2awY4y1VDj0fQngNuHIh71MwBlqSJCm1fE6Gz5YHF7dEmNTTwJ0hhLVjjN+TyUjVCyGsFmP8pUK7JtnnH7LPpUDrHP01qdBmoSwdSpKklVH53KwOVY53BGYCkyu0CxUmx1dsN6qmNzHQkiRJqdXWqsPFkQ2SDgXGZ7NZAG+TWU14WIV2dbLtBsYYy7KHB5CZqL93hXatgB2y5xbJ0qEkSUqtuPZ2hl+d37ZVaAM0CiEcnH09PPv8EJlNRkeTCZJOBHYBji7vJ8b4awjhauDaEML3wAfZdu2AIyu0eyeE8B/gvhDCucAM4CpgAgnmexloSZKk5UkzMvOtKip/fRzQn0ym6tJs2zlkgqgeMcYXKl4UY+wXQgA4E1iHzJYN3WOMH1fp/wigH5nNSeuRuQXPITHGn2sabFFZWVlNbZZL9bc8fcX8YNIy7Ptht9X2EKSVVoN6SzfFtNcdw/L2PfvyadussLeoNqMlSZJSq8Vb8CxXnAwvSZJUIGa0JElSasUmtBIx0JIkSalZOkzG0qEkSVKBmNGSJEmpmdBKxkBLkiSlVoSRVhKWDiVJkgrEjJYkSUrNVYfJGGhJkqTUXHWYjKVDSZKkAjGjJUmSUjOhlYyBliRJSq3YSCsRS4eSJEkFYkZLkiSlZkIrGQMtSZKUmqsOk7F0KEmSVCBmtCRJUmomtJIx0JIkSam56jAZS4eSJEkFYkZLkiSlZj4rmYUGWiGEsUBZyv7KYoztlmxIkiRpWeeqw2QWldEaTPpAS5IkSVkLDbRijL2X4jgkSdJypNiEViLO0ZIkSalZOkwmdaAVQlgF2BhoTI5VizHGIXkYlyRJ0nIvcaAVQigCrgFOB9ZYRNM6SzooSZK0bDOhlUyajNaFwB+Be4EhwD+yx6aRCb7mAhfke4CSJGnZY+kwmTQblh4PPBNjPAUYmD32fozxXqArmUzWznkenyRJ0nIrTaDVGhiU/e952efVAGKMvwKPAMfmb2iSJGlZVVyUv8eKLE3psJRsYAXMAGYDrSqc/wVYK0/jkiRJyzBLh8mkyWh9BmwBEGOcD7wL/CGEsF4IoTVwCjAq/0OUJElaPqUJtB4FOoYQyrNaFwMBGA+MBTbMHpMkSSu4ojw+VmSJS4cxxgeBByu8/m8IoSPQg8ycrZdijF/me4CSJGnZU2zpMJEl2hk+xjgWuCVPY5EkSVqheAseSZKUmgmtZNLsDD8fKKupXYzRneElSVrB1daqwxBCe+A8YBtgU2BUjHHTCufrAOcC3YGOZGKdT4ErY4yDqvQ1DmiT423WjjH+r0K7hsCNwMFkdmB4HTgjxjiupvGmyWhdRfVAqw7QFjgAiMC/U/QnSZKU1iZkgqh3yCzqq7qwrz6ZxXkPkQmO5gC9gVdCCD1ijFVjlX8Cf6lybFqV148DWwFnkNni6ipgUAjhdzHGnxc12DST4a9Y2LkQQgtgGPBF0v4kSdLyqxZLhy/EGJ8HCCE8CHSucn4WsH6MsbT8QAjhZWAjMpmuqoHWtzHGYQt7sxDC1mQCu+4xxgHZY58CY8gEcHcuarBptndYqBjj18BdwGX56E+SJC3biouK8vZII7uX56LOz6sYZGWPlQEfAS1Tf1DYF5jOb7cfJMY4AXgre26R8jkZ/idg/Tz2J0mStMRCCMXAdsDIHKd7hRBOJLNV1X+Bi2KMH1Q434HMPLCqAd4IYO+a3jsvgVYIYVPgTCwdSpK0Ushn6TCEUAKU5Dg1LcZYdb7U4jiDzCbrJ1c53p/MXK8JZCbFXwS8GULoEmP8PNumCdXnbEHm1oRNa3rjNKsOx5J71WEJ0Bj4mcykeEmStILL86rDs4G+OY5fCVyxJB2HEHYGbgD6xRjfrHguxnhmhZdvhhBeJHM7wT8CxyzJ+5ZLk9EaTPVAq4xMRDcGeCLG+EM+BpUPpcNvr+0hSCudJl3PrLmRpIKY9cGttT2EJXEzFe4+U8ESZbNCCJsBzwPPARfW1D7GODWE8BrQqcLhUqB1juZNgBrjnjSrDnsnbStJklZseVlNl5UtD+ajRLhACKEd8BLwAXB0dkL84hgJ7BlCKKrSR0cy2a9FSvxzCiHcn13iuLDzXUMI9yftT5IkLb+Kiory9si3EEJz4GXgG+CAGOPshNetBewODK9weACZaVJ7V2jXCtghe26R0pQOewOvkpk0lsv6wLHA8Sn6lCRJSiyEsDq/bavQBmgUQjg4+3o48B2ZrRiaAf8HdAwhLLi+fM+sEMIRwH7Ai8BkMhuwXwjUA/5cof07IYT/APeFELkdg/0AACAASURBVM7ltw1LJ5C73FlJPrd3WBP4NY/9SZKkZVRx7W1Y2gx4usqx8tfHAW8Am2dfP5fj+vKRjyWzr9Zfycy3mk5mPvrBMcaqJcEjgH5kNietR+YWPIfUtCs81BBohRB2AnapcOig7D2GqmoCHA58XNMbSpKk5V9tBVrZ+wvW9O41ji6b2do14XvOBE7JPlKpKaO1K78ttywDDso+chlBZi8tSZK0gqutm0ovb2oKtG4AbicTGX4H9AGeqdKmDPg5xvhL/ocnSZK0/FpkoBVjnEXm5oyEENYHvssekyRJK7FanKO1XEmzDcbqwO8XdjKE0CuEsPGSD0mSJC3riory91iRpQm0riMz635hDgeuXbLhSJIkrTjSBFrbkFnOuDCvZ9tIkqQVXHFRUd4eK7I0+2iVAD8t4vwvJLiLtSRJWv7l8xY8K7I0P6exwE6LOL8TmV1SJUmSRLqM1qPAlSGE4cCtMca5ACGEusBZwCHAn/I/REmStKxZwSt+eZMm0Loe2JHMFvQXhxC+yB7fiEzJcBBOhpckaaWwos+typfEpcMY4xxgHzI3jR5KZs5WSfa/jwP2AloXYIySJEnLpVQ3lY4xlpG5U/WD5cdCCGuR2dphKNAFqJO/4UmSpGWRCa1kUgVa5UII9YEDgKOAPYBVgC+Bv+RvaJIkaVnlzvDJJA60QghFwJ5kgqsDgAZk7nN4H/CXGGMsyAglSZKWUzUGWiGETmSCq8OA5mQyV38FhgMvAAMNsiRJWrk4GT6ZRQZaIYSRZFYVTiazvcPjMcYPsufaFX54kiRpWWSclUxNGa1AZqPSPwL9Y4y/Fn5IkiRJK4aaAq0TgV7A48BPIYTns//9cqEHJkmSll1Ohk9mkYFWjPF+4P4QwrpkAq5eZOZrTQUGk5kMX1boQUqSpGVLEUZaSSRadRhjnAzcANwQQtiMTLB1OFAE3BVC2B/oD7wSY1zUjaclSZJWGqlvvh1j/CTGeAHQBtgd+A9wEPAv4Pv8Dk+SJC2Liovy91iRLdaGpbBgl/jXgddDCH8AepIpLUqSpBXcih4g5ctiB1oVZVcjPpV9SJIkiTwFWpIkaeVS5EZaiRhoSZKk1CwdJpN6MrwkSZKSMaMlSZJSs3KYjIGWJElKzZtKJ2PpUJIkqUDMaEmSpNScDJ+MgZYkSUrNymEylg4lSZIKxIyWJElKrRhTWkkYaEmSpNQsHSZj6VCSJKlAzGhJkqTUXHWYjIGWJElKrbY2LA0htAfOA7YBNgVGxRg3zdGuG3AN0BGYDNwcY7wtR7vzgNOA5sAI4MIY46AqbRoCNwIHA6sBrwNnxBjH1TReS4eSJGl5sgnQHRgNfJ6rQQhhW6A/8CHQDXgAuDmE0KdKu/OAa4E7sn1+CfwnhLB5lS4fB3oAZwCHAS2BQSGE1WsarBktSZKUWi1Ohn8hxvg8QAjhQaBzjjaXAx/EGE/Ivn49hNAa6BtCuCfGOD+EUA+4lEymq1+2v8HAp8AlwKHZY1uTCcK6xxgHZI99CowBegN3LmqwZrQkSVJqxUVFeXukEWOcv6jz2QBqN+DJKqceI1Me3Cr7ejugMfBEhb7nAU8B3UII5QPbF5gODKzQbgLwVvbcIhloSZKkFUk7YFWqlxVHZJ83zj53yD6PzNGuAbBuhXajcgR4Iyr0tVCWDiVJUmr5LB2GEEqAkhynpsUYp6Xsrkn5tVWOl2afm1Zo92uMcdYi2k3Ktss1htIKfS2UGS1JkpRacR4fwNnA2ByPs5fOpykcM1qSJKm23Qw8mON42mwW/JaRqpohK890/VChXb0Qwmoxxl9qaNc6x/s0qdBmoQy0JElSakV5rB1my4OLE1TlMgaYTWZu1cAKxztmn0dln8vnZnUgsw1ExXYzyey9Vd5uzxBCUYyxrEq7UdTA0qEkSUqtKI+PfIox/gq8RnZ7hgqOAL4BPsi+fpvMasLDyhuEEOpkrxtYIagaQCY7tneFdq2AHbLnFsmMliRJWm5kNwkt31ahDdAohHBw9vXwGON44CpgSAjhXuBRYHvgJOC08tWDMcZfQwhXA9eGEL4nE4CdSGbV4pHl7xdjfCeE8B/gvhDCucCMbP8TyF3urMRAS5IkpVZbt+ABmgFPVzlW/vo44MEY49AQQk8yu74fA0wBzokx3lXxohhjvxACwJnAOmS2bOgeY/y4Sv9HAP3IbE5aj8wteA6JMf5c02CLysrKamqzXPplLivmB5OWYU26nlnbQ5BWWrM+uHWpRj6Pvj8pb9+zvTqtt8Leoto5WpIkSQVi6VCSJKVWi/c6XK4YaEmSpNTyub3DiszSoSRJUoGY0ZIkSamZqUnGQEuSJKVm6TAZAy1JkpSaYVYyZv4kSZIKxIyWJElKzdJhMgZakiQpNUtiyfhzkiRJKhAzWpIkKTVLh8kYaEmSpNQMs5KxdChJklQgZrQkSVJqVg6TMdCSJEmpFVs8TMTSoSRJUoGY0ZIkSalZOkzGQEuSJKVWZOkwEUuHkiRJBWJGS5IkpWbpMBkDLUmSlJqrDpOxdChJklQgZrQkSVJqlg6TMdCSJEmpGWglY+lQkiSpQMxoSZKk1NxHKxkDLUmSlFqxcVYilg4lSZIKxIyWJElKzdJhMgZakiQpNVcdJmPpUJIkqUDMaK1kpk0r5bVXX2XIkDcY/cUXfPfdt6yyyiq033Ajeh54EAcc+HuKi5PH30MGv8GjjzzMV2NGM33aNNZae206dtyEo489js232LKAnySdMaNHc9edtzF8+Lv89OOPtGjZkn26def4E09mtdVWq9R2/PhxDHrlZd5+679MmDCeqf+bSqPGjdhss83pdfSxdN16m1r6FFqeNW28Oj123ZxuO3Rkkw1b0nLtxsyeM48Ro6fwcP93eLj/O5SVlS21fpaWjddvzqWndGPHzu1ptMZqTPj6B55++QP6PfAqv/w6p1Lb9dYp4bzj9mSrDq1o1aIpTRqtzg/Tf+KrSf/joeeH8fiA4cydO7+WPomqsnSYTNGy9Bcyn36Zy4r5wZbQU08+zjVXXcHaa69Nl65b07xFS6ZO/R+vvfoKM2fOZI8996bfTbdQlCAnfNNfbuTB+/9OSUkJu+6+ByUlTZg4YQJvvP4a8+bN5errrme//XsuhU+1aJ988jEnHX8sc+fMZc+99mad5s0Z/s4wRoz4jC223Ip773+IVVdddUH7C847h5deHMAG7dqz5VadaNy4MePGjWXw668xb948LrjoEnoddUwtfqJlV5OuZ9b2EJZZJ/5+e2675DC+/n46g4d/ycRvSmm2ZkN67rYZJQ1X59lXP+LIC+5fav0sDV02bcOLd5/OKnXr8OyrHzHp22ns0mVDOm3Shrc/HEO3Pncwe87cBe137NSep/96EsM/G8/Yyf+jdMbPNG28Bntv14FWLZryxvAv2O/UO5k3z2Arl1kf3LpUI58hX/yQt+/ZnTZqusJGbQZaK5l3hg1l1qxZ7LTzLpUyV//7/nt6HX4I33zzNX+56Vb22GvvRfbzv++/Z8/ddqJJk6Y8/Wx/1lxzzQXn3n1nGCcdfyzrrrceA14alPfPMPzddzjxuGO46urr6HngQYtsO2/ePA4+YH+++moMt9x2J7vstjsA8+fP5/z/O5tXX3mJM88+lxNOOnnBNc8/+y822nhjOnToWKmv94a/yyknHk9REbz4ymusvXazvH+25Z2B1sLt3GVD1qhfjxffHFEp47TOmg158+FzadWiKUecdx/PvfbxUulncezYqT0v33smJ/V9hEdeeHeRbYuLi3jvqYvosEFzDj77Hv4z5DMAioqKePT64zhwjy247Nb+9Hvw1QXXrFK3DnPnza+Wkatbt5h/33EaO3fZkKMufIBnXvkw759tRWCgtWxyjtZKZutttmWXXXerVh5ca+21OeSwwwEYPnzRv0ABpnw9hfnz5/O7zTarFGQBdN16G9ZYYw1Kf/gh57Uv/uffnND7aHbYpjNdtvwdB+zfjXvuupPZs2cv5qdauPeGv8tXX42hU+cuC4IsgOLiYs4593wA/vnUE5V+sfc88KBqQRZA5y5d6dy1K3PmzOHjD/1Fr3QGD/+SAUM+qxZEfDt1Jvc+8xYAO3VuX9B+Dt17KwbefQZfD/4zpUP/wofPXMyFJ+zFqqvkfxbJjp3a02GD5rz5/ugFQRZAWVkZF9/yPAAnHrx9pWvmzJ2Xs+w5d+58XnjjEwDatV4772PV4inK4/9WZAZaWqBu3cwv27p16tTYtk3rNqyyyip89umnlJZWDqjef284P/30E1tvu1216y6/9CL+eMG5TJw4gd333IvDjuhF40aNueO2W/jDyScwd+7catcsieHvDANg+x12rHZuvVataNO2LVOmTGbSxImJ+lsl+zOqU7fmn5GU1Ny58zLPS1gSW1Q/d/U9koeu6027Vmvx3KCPufupNymd/jNXnLYf/W//A3Xq5PfrYJcuGwHw8tsjq50bN3kqX4z7ljYt12T99daqsa/i4iL23iHzj5/PvpyS13Fq8RUV5e+xIlvmJ8OHEFoDu8QYH67tsazI5s6dywv9M//K3C5HUFJV45ISzv6/8+h3w585sEd3dtttDxqXlDBpYmaO1jbbbc9lfa+qdM3zz/6L55/9F7vtsSfXXd+v0iT0v91xG3fdeTtPPv4ovY4+Nm+fa9y4sQC0adM25/nWbdoyftw4xo8fS6vWrRfZ15Qpk3ln2FBWq1+fTp265G2MWrnVqVPMkd27ArmDknz0c9T+XTm25zY8/9rH9L7k4UqT0C85pRuXntKNPofuyB2PD17s969qozaZ0vro8d/lPD964vds1HYdNmy9NmMn/a/SuTVL1qDPYTtRVARrlTRg920C7Vs344kB7zGgQnZMK6cQwhvAzgs5fVGM8c8hhCuAvjnOnx9j7Felv2OAi4G2wBjgqhjjk/ka7zIfaAFdgAcAA60CuuWmvzD6yy/Ycaedc2Z/cjnqmN60XHc9+l56Mc/886kFx1u3bkPPngdWKyk++sjD1K1blyv/dG21lX4n9zmVJx57hP/8+4W8Blozf/wRgAYNG+Y837BBg0y7GTMX2c/s2bO56ILzmD17Nuecez6NGjfO2xi1crv6jP3ZdMOWvPjmCF4dOqog/Zx2xC7MmTOPU654rNpKv+vuHUifQ3fk8G6d8xpoNWpQH4DpP87KeX7GzF8AKGlYv9q5NUvW4NJTui14PX/+fG56eBCX3/5C3sanJVeLiahTgUZVjh2dPT6gwrFZwG5V2o2v+CKEcDDwEPBn4GXgAODxEMKMGOOL+Rjs8hBoqcAefeRhHn7wftbfYAOuue6GxNc9cN+93HbLTRzR62iOOPIo1lxrLcaN/YpbbvorF114HnHUSM457wIAZs2axRdxFCVNmvDoPx7K2d8qq67K2K/GVDp2Qu+jeW8hc8Yuv/QiLr/0okrHOnfpyn0P/iPxZ0hi3rx5XPLH8/noww/Yu9u+HHvcCXntXyuvUw/fibOP2Z1RY7/hhMsW/8/tovqpv9oqbLZRS/437SdO77VLzutnz5lLWH+dSsdeuucMduq8Yc729155FPdeeVSlY0Pe+5K9T75tsT9DRV+M+476W51JcXER6zYroceum3FZn33ZbosNOPDMuymd8XNe3kdLpriWan4xxs+rHgsh3Ap8GmP8pMLh+THGYTV09yfg6Rhj+ZfJ6yGEDsCVwPIdaIUQPqm5FVA9alUePf7oI9xw3TVs0K499973II1LShJdN/zdd7j5r/3YbY89Of/C34KdDh034aZbb6dH9715+KEHOOSwI1ivVStmzJhBWVkZpT/8wF133p54fD16HkjnLl0rHZsyeTL9n3+WXXfbnbBxh0rnWrZct9Lr8ozVjzNzZ6zKM14NG+XOeM2bN4+LLzyfl18ayF77dOPaP9+YaOsLqSZ9DtuRv1xwMJ+P+Zp9+9y+2MFDTf00abg6xcXFNGvasFKWqCb/eOEdhrw/utKxNi2acnSPrXnh9U/4+IvJlc6NnzK10usZ2UxW4wbVM1YAjRpmstrTZubOeAHMn1/GxG9KuePxwXw3dSYP/7k3l/9hX865/p+JP4dWfCGEDclUvy5Med36wMbAJVVOPQY8EEJYO8b4/ZKOrzYzWh2AEUBNy7faAK0KP5yVzyMPP8iN119H+w034p77HqxW6luUIYPfAKBL162rnatfvz6b/m4zXnv1FUaN/Jz1WrVaEPBs3KEjT/7z2cTvk2v7huHvvpMNtPaocXuHtm3XBzKbkOYyIXu8TZv1q52bM2cOF194Hi+/NJB9u+/H1dfdQJ0ECwWkmpx+5C7ceN5BfPblFPbtczvfl/5YsH7KS3cfjpzIdr1uTNx3ru0bduzUnqN7bE3/Nz6pcXuHL7Jzs9q3yb0NSvtWmdWDX05I9j320tufLxiDlg35/CdnCKEEyPUv/Wkxxmk1XH4UMJ9MgFRR/RDCd0BTYDRwW4zxjgrny/+lXjVDNqJ8WMASB1q1uerwM+CLGONxi3oAd9TUkdK7/+/3cOP11xE27sDfH3goVZAFLNiKYWFbOJQfr7vKKgCsvsYatGu/IWNGf8n0aTX9ncmfLtld3N/675vVzk2aOJHx48bRsuW6rNeqciw/Z/Zszv+/s3j5pYHs3+MArvnzjQZZyotzj92DG887iI9GTWKfU25b7CAraT8/zZrNiNFT6NiuBU0arb4kQ0/ljeFfALDXdh2qnWu77pps1HYdxk+ZWm0i/MK0XDvzHbykKzOVR0V5fMDZwNgcj7MTjKQXMDjGOKnCsdFkMlxHAD2AocDt2Uny5Zpkn6t+KZVmn5smeO8a1Wag9Q5QPR2Sm7WaPLr7b3dwy01/oeMmm3DvfQ/SpMnC/yzNmTOHsV+NYeKECZWOb9WpEwDPPP0U3377baVz/31zMB99+AH16tVjiy1/uw3P0cf2Zs6cOfS97GJmzJhR7b1mTJ/OyM9HVDu+JDp36coGG7Tj/feG88Zrv22eOn/+fG7+a+Zf9wcfenilcuDs2bM556zTef21QRz4+4O56prrUt2WSFqYP564N1ef1YP3P5/Avn1uZ+q0nxbatm7dYjZq2yzn9gdp+gG49dE3qLdqXe7ue2TOUl5Jw/pssfF66T/QIrz5/mhGfvUNO3ZqT/edNl1wvKioiGvO7AHA3//5VqVrtth4PYqLq/+6X6P+qvQ7P5O9Hvjf/P6O0DLjZmD9HI+bF3VRCGEboB3wSMXjMcZHYoz9YoyDYowDsombB4ELQwhrFGD8C1WbpcMbqbw6YGEGkPlhKw/6P/csd95+K3Xq1GGrrTrz2KPVJ+C2bLnugpLcd999ywH770vLluvy4iuvLWiz5177sM22TzNs6NscuH83dtt9T9Zcay3GfjWGIYPfoKysjLPOOZeSkiYLrjnwoIMZOWIETz7xGPvtsyfbbb8DzVu0YMb06UyePIn33xtOzwMPqrYtxJKoU6cOV15zHScdfyznnnMWe+61N81btODdYUMX3ILn6GN7V7rmT1f25c0hg2nSpAnNmq3D3X+rnlTt3KVrzrKptDC99utK31O7M3fuPN7+cAynHlF9dfr4KVMXlORarl3Cx/+6lPFTprLxflcudj8ADz8/jC07tKLPoTsyYst2vDp0JBO/KaVJ49Vp23JNdtiqPQ/3H8aZ1z5Vra/FNX9+Gadc8Sgv3n06j914PM+++hETvyll164bLbgFz62PvlHpmotP2odtttiAYR+PZdI3P/DzL3NYb50S9tq+I00arc7Qj77ixvtfydsYtWTyudFotjy4OOWOo4BfgCQT954CegMdgeH8lrkqAb6p0K78iyt3ySalWgu0YoxjyOxXUVO7WVRZjqnFN3lyJrM6b948HlnI6r/OXbrWOPepuLiY2/92D08+/igDXxzAa4Ne4ZdffqFR48bssNPOHNnraLbbfodq1118WV+233Ennn7qCYYNe5uZM2bSuHFjmrdoQe/jTqD7/j2W/ENWsdlmm/PYE//kb3fcytC3/8tPP/1Ei5brcsofTuP4E0+udJ9DgCnZn1FpaWnOIAugz6mnG2gplbbrZsrzdevW4Yxeu+ZsM+S9L2uc+7S4/Zzz56d5+a3POfHg7dl160BJw/r8MP1nJn1Tyk0PD+LxAcPTfqQaDf9sPDsc9Rcu69ON3bfZmIbZm0pfc8+L9Hvg1Ur3OQS4/9m3+XHWr3TepA07dWrP6qutSunMn/lw5ESeeeVDHnp+mPc5XIbU9rqgEEJd4DDghRhj9TJJzco3nOsAVNwTpfzWIHEJhreA9zqUlDfe61CqPUv7XofvfjU9b9+zXTdonHrsIYTuwL+BnjHG/gnaPwQcAqwVY/w5e2wk8HGM8fAK7QYCTWOMXXP3lI77aEmSpNSWgcnTRwFTybHfVQjhfTIbkUZgVTKZr17ApeVBVtblwJMhhDHAK0BPYC+ge74GaaAlSZLSq8VIK4TQgMxqwodijHNyNBlNZsVii+zrEcDxMcYHKjaKMT4dQlidzC14ziMzpenIfO0KD5YOJeWRpUOp9izt0uHwsfkrHXZZP33pcHlhRkuSJKWWz1WHKzIDLUmSlFptrzpcXrgLoyRJUoGY0ZIkSamZ0ErGQEuSJKVnpJWIpUNJkqQCMaMlSZJSc9VhMgZakiQpNVcdJmPpUJIkqUDMaEmSpNRMaCVjoCVJktIz0krEQEuSJKXmZPhknKMlSZJUIGa0JElSaq46TMZAS5IkpWaclYylQ0mSpAIxoyVJktIzpZWIgZYkSUrNVYfJWDqUJEkqEDNakiQpNVcdJmOgJUmSUjPOSsbSoSRJUoGY0ZIkSemZ0krEQEuSJKXmqsNkLB1KkiQViBktSZKUmqsOkzHQkiRJqRlnJWPpUJIkqUDMaEmSpPRMaSVioCVJklJz1WEylg4lSZIKxIyWJElKzVWHyRhoSZKk1IyzkrF0KEmSVCBmtCRJUnqmtBIx0JIkSam56jAZAy1JkrTcCCH0Bh7IceqOGOPpFdp1A64BOgKTgZtjjLfl6O884DSgOTACuDDGOChf43WOliRJSq2oKH+PxbQPsG2FR7/yEyGEbYH+wIdANzKB2c0hhD4VO8gGWdcCdwDdgS+B/4QQNl/sUVVhRkuSJKW2DBQO348x/m8h5y4HPogxnpB9/XoIoTXQN4RwT4xxfgihHnApmUxXP4AQwmDgU+AS4NB8DNKMliRJWmFkA6jdgCernHqMTHlwq+zr7YDGwBPlDWKM84CngG4hhLzEkma0JElSerWf0voshLA2MAF4ELgmxjgXaAesCnxepf2I7PPGwHtAh+zrkTnaNQDWBSYt6SANtCRJUmr5XHUYQigBSnKcmhZjnFbl2NdAX+BdYB6ZOViXAesDvYEm5ddWua40+9w0+9wE+DXGOGsR7Qy0JEnScu9sMsFTVVcCV1Q8EGN8CXipwqFXQgjTgStCCH8q2AgXk3O0JElSanledXgzmYxU1cfNCYfzVPZ5K37LSFXNkJVnun7IPpcC9UIIq9XQbomY0ZIkSanlc4pWtjxYtdS3uMYAs8nMwRpY4XjH7POo7HP53KwOZLaBqNhuJpm9t5aYGS1JkrS8OxwoI7Plw6/Aa1TfnuEI4Bvgg+zrt4HpwGHlDUIIdbLXDYwxluVjYGa0JElSakuw0egSCSG8RCaQ+gyYT2Yy/KnAfTHGr7LNrgKGhBDuBR4FtgdOAk6LMc4HiDH+GkK4Grg2hPA9mQDsRDKrFo/M13gNtCRJ0mKotf0dRgLHA+uRiWO+BC6kwnyuGOPQEEJPMru+HwNMAc6JMd5VsaMYY78QAsCZwDpktnboHmP8OF+DLSory0tmbJnzy1xWzA8mLcOadD2ztocgrbRmfXDrUo18JpXOztv37HpNVq39XbkKxIyWJElKrbZKh8sbAy1JkpSacVYyrjqUJEkqEDNakiQpNUuHyRhoSZKk1PJ5r8MVmaVDSZKkAjGjJUmS0jOhlYiBliRJSs04KxlLh5IkSQViRkuSJKXmqsNkDLQkSVJqrjpMxtKhJElSgZjRkiRJ6ZnQSsRAS5IkpWaclYylQ0mSpAIxoyVJklJz1WEyBlqSJCk1Vx0mY6AlSZJSM6OVjHO0JEmSCsRAS5IkqUAsHUqSpNQsHSZjRkuSJKlAzGhJkqTUXHWYjIGWJElKzdJhMpYOJUmSCsSMliRJSs2EVjIGWpIkKT0jrUQsHUqSJBWIGS1JkpSaqw6TMdCSJEmpueowGUuHkiRJBWJGS5IkpWZCKxkDLUmSlJ6RViKWDiVJkgrEjJYkSUrNVYfJGGhJkqTUXHWYTFFZWVltj0GSJGmF5BwtSZKkAjHQkiRJKhADLUmSpAIx0JIkSSoQAy1JkqQCMdCSJEkqEAMtSZKkAjHQkiRJKhADLUmSpALxFjxaJoQQNgRuA3YAZgFPABfGGH+u1YFJK7gQQnvgPGAbYFNgVIxx09odlbTiMNBSrQshlACvA+OBg4FmwF+BtYHDa3Fo0spgE6A78M7/t3evMXJWdRzHv4sCglG7lqKYaEsI/SOCvNGgeKloGq3G1KrxlkisBE0Um1AjgqlQSmK8Gy9RQdGGN0Iq4oVoy8VuQAmIXFQo/NFSWmzQLLZVo1Baur44Z7rDuJ21OM/Ozu738+bpnDnPPP+ZZJNfzznPeSizHM50SD3kH5Smgw8Dw8DSzFyfmZcDK4B3R8RL+luaNOP9LDNfmJnvBO7odzHSTGPQ0nTwZuCGzHykre0qYDewpD8lSbNDZu7rdw3STGbQ0nTwYmBTe0Nm7gY2Ayf0pSJJknrAoKXpYBjYNUH7TuC5U1yLJEk9Y9CSJElqiEFL08FOYM4E7cPAjimuRZKknjFoaTq4l7JOa7+IOBw4DrivLxVJktQDBi1NBz8H3hARc9valgGH1/ckSRpIQ2NjY/2uQbNc3bD0buBB4GLGNyy9ITPdsFRqUEQcSdliT/f0KgAABSdJREFUBeCjlJHklfX1bZm5tS+FSTOEO8Or7zJzV0S8Hvga8CPGH8Fzbl8Lk2aHo4F1HW2t18uBtVNajTTDOKIlSZLUENdoSZIkNcSgJUmS1BCDliRJUkMMWpIkSQ0xaEmSJDXEoCVJktQQg5akpyQiRiJipO31gogYi4gP9K+qJ4uI1RHhHjaS+sYNS6UBVQPN99uangD+AlwHrMrM7f2o62BFxInAu4C1mflgn8uRpJ4yaEmDbzWwGXgG8CrgDGBRRJyUmf+ewjq2AkcAew7yvBOBC4ERymOYJGnGMGhJg29DZt5S//3diNhBeVbdUuAHnZ0j4pmZ+a9eF5GZY8Bjvf5cSRpkBi1p5vklJWgdGxFrgfcAJ1CeJfk64I56JCLeB5wDnEQJSdcD52bmlvYPjIgPAZ8EXgD8Afh450UjYgGwBViemWvb2o+hjLq9BZgHPEyZ3lwJvIPx6c+NEdE6bf9nRMTLgYsoo3WHAbcDn87MjR3XfzXwFeBkYDvw+cl+KElqmkFLmnmOq8e/1eMhwLXAb4BPAHsBIuI84DPADylhZxg4G/h1RJySmaO135nAJcDNwFeB+cBPgJ3AQ90KiYjn1+seBVwK3EMJa8uAucCNlAC4otZybz315nr+ImADcBewhjIt+X7g2ohYnJkjtd/J9TuOUkLd0yjTkaP/208mSc0waEmD7zkRcRTja7QuAB4FrgFeCRwKXJOZK1snRMSLgIuB1Zm5pq39CkoYOgf4VEQcSglAdwGnZ+bjtd89wGVMErSAz1KC1WmZeWtb++qIGMrMsYi4iRK0rmsFp3qNIUrA+xWwuE5NEhHfBu6sdZ1Wu6+hBMrXZOa22m9d/S6S1DcGLWnwre94vQlYkZnb26bivtnR5+2Uv/8ra0hr+TtlavD0+vplwNHARa2QVV0OfKlbURFxCGXk6hcdIQvYv6arm1OAoEwBzm37LlCmHj8WEUcCu4E3Aj9thaz6+fdHxAbKlKUk9YVBSxp8KyhTbo8B24CHOkLMPv77br6F9XjfAT7zgXqcX49/bH8zM/dGxBa6mwc8G7h7kn4H0qrxsi595lKmE4/orLG6H4OWpD4yaEmD77a2uw4nsicz93a0tTYrXkJds9Xh0Z5U9v9p1XgeZQH8REaBOVNTjiQdPIOWNDttrsdtmbmpS7+t9Xg8ZboOgIh4OnAs8Lsu544C/6Dc0djNgaYQWzX+MzOvP9DJETFKCYbHT/D2wgnaJGnK+AgeaXa6irKT/AV10fmTtK3b+i0lMJ0VEYe1dTmDSUaSMnMfcDWwJCJOneAareu29vQa7uhyO/AnYGVEPGuC8+fV6zxBuTPxrXWRf+v9hZS1W5LUN45oSbNQZj5Qt3f4AjA/In4M7KKMUi0FrqTckbgnIlZR7v7bWO9KXAAsZ3wdVzfnA4uBkYi4hLJQ/3mUxfjLKGvH7qSEvvMjYg5ldOrWzNxSt5ZYD2yKiO8Bf6bcxbgIGGJ80f6FwJuAmyLiW5T/RJ5dr/fSp/QjSVIPOKIlzVKZ+UXgbcDjwCrgy5TwMwKsa+t3KfAR4BhKMHstJYxNtrUDmfkwcCpwBfBe4OvAByl7az1S+/wVOIsyovUdym72i+p7NwKvAG6pNXyjnr8D+FzbdX5PGb0apWxuemY9Xn1QP4ok9djQ2JgPtpckSWqCI1qSJEkNMWhJkiQ1xKAlSZLUEIOWJElSQwxakiRJDTFoSZIkNcSgJUmS1BCDliRJUkMMWpIkSQ0xaEmSJDXkP9Wmb/ZPx3tfAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4SbVauNR8fF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "f123a5d4-01d6-4e31-fafe-dd4b2e262a55"
      },
      "source": [
        "df_cm\n",
        "# our model predict 2204 FN and 2189 FP"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Predicted</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Actual</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2204</td>\n",
              "      <td>326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>281</td>\n",
              "      <td>2189</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Predicted     0     1\n",
              "Actual               \n",
              "0          2204   326\n",
              "1           281  2189"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRh7SpypThjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}